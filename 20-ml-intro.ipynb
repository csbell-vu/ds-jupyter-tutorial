{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An intro to using Python in Jupyter notebooks for data science\n",
    "## Introduction to machine learning tasks in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Purpose**: The purpose of these repository notebooks is to perform a classification task on the selected dataset to classify whether, according to some predictors, a person's income will exceed $50k.  The dataset includes several predictors, but the ones that we will use for classification are `hours-per-week` (how many hours are worked per week), `workclass` (broad category of the type of work performed by the person), and `age` (how old the person is, in years). \n",
    "\n",
    "**Notebook purpose**: This notebook demonstrate some sample steps of a machine learning task.\n",
    "\n",
    "**Data**: In this notebook, we will used the cleaned data (at data/adult_clean.csv) created from the previous notebook.  All the columns of the original data set are maintained, but all NAs are removed/replaced.\n",
    "\n",
    "**Preprocessing tasks**:  After the data is cleaned, I preprocess the variables of interest in the following ways:\n",
    "- **numerical predictors** (`hours-per-week`, `age`): A standard scaler is applied to these.  More details are given in the appropriate section.\n",
    "- **categorical variables** (`workclass`, `salary-class`): These are pre-processed into one hot features.  For the current ACCRE version scikit-learn, this first requires converting the string-encoded categories into numerical values (e.g., 'a', 'b', 'c' -> 1, 2, 3).  The one hot encoder then converts this to one hot.\n",
    "\n",
    "**Modeling tasks**:  The data is modeled using a logistic regression classifier with 5-fold cross validation.\n",
    "\n",
    "**Performance evaluation**: The confusion matrix and other metrics including precision, recall, and area under the curve (AUC) are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for data frame behavior\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will load the data.  Best practices and helping to ensure a hardened pipeline require that some checks are done on the data after it is loaded.  This will be discussed at length during the semester.\n",
    "cleaned_data_filename = 'data/adult_clean.csv'\n",
    "df = pd.read_csv(cleaned_data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the data: Explicitly performing each individual step\n",
    "In this step, we'll model the data using some parts of the data as predictors and one column as the response.  We will explicitly perform each step of the pipeline to understand each step and its purpose.\n",
    "\n",
    "For the predictors, we will use \\[age, workclass, hours-per-week\\], and for the response, we will use salary-class.  Some of these variables will require some preprocessing to get them into a form suitable for a classifier, which will be detailed later.\n",
    "\n",
    "Note the importance of splitting the training and testing set prior to performing any preprocessing (normally also including imputation).  This ensures that we don't inform the model about behavior of the testing set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into training and testing sets.  This is essential so that information that should not be known to the testing set is not known.\n",
    "\n",
    "#Split data into relevant portions\n",
    "data_y = df['salary-class']\n",
    "data_x = df[['age', 'hours-per-week', 'workclass']]\n",
    "\n",
    "ptrain_x, ptest_x, ptrain_y, ptest_y = train_test_split(data_x, data_y, train_size=0.85, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings for predictors and response variables\n",
    "Here we encode and preprocess the data in a way that the package expects the inputs to be received.  The preprocessing will be as follows:\n",
    "* **Predictors**:\n",
    "    - workclass: One-hot encoding.  Note that for backward compatibility with scikit-learn and pandas, the onehot encoder must be preceded by a labelencoder (i.e., one cannot directly encode strings with OneHotEncoder for this version of scikit-learn.)\n",
    "    - age: Standard Scaling (i.e., $\\frac{(x-\\mu)}{\\sigma}$)\n",
    "    - hours-per-week: Standard Scaling (i.e., $\\frac{(x-\\mu)}{\\sigma}$)\n",
    "* **Response variable**:\n",
    "    - salary-class: Binary encoding (e.g., $<$50k is class 0, >=50k is class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode workclass; note that if we wanted to, we could one hot all of the categorical matrices\n",
    "pre_wc_encoder = LabelEncoder()\n",
    "wc_le = pre_wc_encoder.fit_transform(ptrain_x['workclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "wc_1h = wc_encoder.fit_transform(pd.DataFrame(wc_le))\n",
    "wc_df = pd.DataFrame(wc_1h, columns=pre_wc_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wc_df.shape)\n",
    "wc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrain_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale inputs to logistic regression\n",
    "num_scaler = StandardScaler()\n",
    "ns = num_scaler.fit_transform(ptrain_x[['age','hours-per-week']])\n",
    "ns_df = pd.DataFrame(ns, columns=['age', 'hours-per-week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ns_df.shape)\n",
    "ns_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings for responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary encode label\n",
    "sc_encoder = LabelEncoder()\n",
    "sc = sc_encoder.fit_transform(ptrain_y)\n",
    "train_y = pd.Series(sc, name='salary-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptrain_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_y.shape)\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all of the features together\n",
    "#feat_df = df[['age', 'capital-gain', 'capital-loss', 'hours-per-week']].copy()\n",
    "train_x = pd.concat([wc_df, ns_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_y.shape)\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a classifier (via logistic regression)\n",
    "lr_classifier = LogisticRegressionCV(class_weight='balanced', solver='lbfgs', max_iter = 1000, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train classifier via kfold cross validation\n",
    "lr_classifier.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use above encoding methods to create test set\n",
    "wc_le_test = pre_wc_encoder.transform(ptest_x['workclass'])\n",
    "test_x = pd.concat( [pd.DataFrame(wc_encoder.transform(pd.DataFrame(wc_le_test)),\n",
    "                                 columns=pre_wc_encoder.classes_),\n",
    "                     pd.DataFrame(num_scaler.transform(ptest_x[['age', 'hours-per-week']]),\n",
    "                                  columns = ['age', 'hours-per-week'])], axis=1)\n",
    "test_y = pd.Series(sc_encoder.transform(ptest_y), name='salary-class')\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(test_x.shape)\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y.shape)\n",
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the classifier on the held out test set\n",
    "pred_y = lr_classifier.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Can use a classification report to get other metrics:\n",
    "print(\"Classification report: \\n\", classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate popular singular metrics\n",
    "roc_auc_score(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, visualization provides a more intuitive understanding of the results.  For a better vis of the confusion matrix, we can use the matplotlib and seaborn packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the confusion matrix of the result of the testing set\n",
    "conf_mat = confusion_matrix(test_y,pred_y)\n",
    "conf_mat_ratio = conf_mat/(pred_y.shape[0])\n",
    "print(\"Confusion matrix: \\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1) \n",
    "ax = sn.heatmap(pd.DataFrame(conf_mat, index=sc_encoder.classes_, columns=sc_encoder.classes_),\n",
    "          annot=True, fmt = \"d\", annot_kws={\"size\": 14}, cbar=False)\n",
    "ax.set_xlabel('Predicted class', fontsize=16);\n",
    "ax.set_ylabel('Actual class', fontsize=16);\n",
    "ax.set_title('Salary-class Confusion: Counts')\n",
    "\n",
    "plt.subplot(1,2,2) \n",
    "ax = sn.heatmap(pd.DataFrame(conf_mat_ratio, index=sc_encoder.classes_, columns=sc_encoder.classes_),\n",
    "          annot=True, fmt = \"0.2f\", annot_kws={\"size\": 14}, cbar=False)\n",
    "ax.set_xlabel('Predicted class', fontsize=16);\n",
    "ax.set_ylabel('Actual class', fontsize=16);\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "ax.set_title('Salary-class Confusion: Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Data via Pipelines\n",
    "One extremely valuable behavior of scikit-learn is to provide pipelines.  This essentially defines a set of steps that should be taken for any data that will be input to the model.  This enables a simple description of what should be performed on each part of the data, and additionally restricts the ability of training information to be used on the testing data.\n",
    "\n",
    "One drawback of pipelines in general is that they may fail silently.  However, as scikit-learn is both open-source and built on Python, additional functionality (e.g., transformer types) may be created by the developer to attempt to combat such challenges."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Define parameters of the pipeline\n",
    "num_preds = ['age', 'hours-per-week']\n",
    "cat_preds = ['workclass']\n",
    "res_var = ['salary-class']\n",
    "\n",
    "#Define steps to be done for each type of variable defined\n",
    "num_steps = [('scaler', StandardScaler())]\n",
    "cat_steps = [('encoder', OneHotEncoder(handle_unknown='ignore'))]\n",
    "res_steps = [('encoder', LabelEncoder())]\n",
    "\n",
    "#transfomers\n",
    "num_xformer = Pipeline(num_steps)\n",
    "cat_xformer = Pipeline(cat_steps)\n",
    "\n",
    "#Defined preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[('num', num_xformer, num_preds),\n",
    "                                              ('cat', cat_xformer, cat_preds)])\n",
    "\n",
    "#Create full pipeline steps\n",
    "lr_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                               ('classifier', LogisticRegressionCV(class_weight='balanced', solver='lbfgs',\n",
    "                                                                   max_iter =1000, cv=5))])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Split data into relevant portions\n",
    "data_y = df['salary-class']\n",
    "data_x = df[['age', 'hours-per-week', 'workclass']]\n",
    "\n",
    "ptrain_x, ptest_x, ptrain_y, ptest_y = train_test_split(data_x, data_y, train_size=0.85)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Perform encoding on train_y and test_y\n",
    "res_encoder = LabelEncoder()\n",
    "train_y = res_encoder.fit_transform(ptrain_y)\n",
    "test_y = res_encoder.transform(ptest_y)\n",
    "res_encoder.classes_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Perform prediction on the data using the pipeline\n",
    "lr_pipeline.fit(ptrain_x, train_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = lr_pipeline.predict(ptest_x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(classification_report(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Look at confusion matrix\n",
    "conf_mat = confusion_matrix(test_y, y_pred)\n",
    "conf_mat_ratio = conf_mat/(y_pred.shape[0])\n",
    "conf_mat = confusion_matrix(test_y,y_pred)\n",
    "print(\"Confusion matrix: \\n\", conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
